{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37b849d-d8e3-4c31-b353-ec0c72f83224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 05:09:42,936 - INFO - Starting dataset creation...\n",
      "2025-01-19 05:09:42,937 - INFO - Processing region 1/1: 10.7449, 92.5\n",
      "2025-01-19 05:09:49,755 - ERROR - Error fetching green cover data for 10.7449, 92.5: module 'osmnx' has no attribute 'geometries_from_point'\n",
      "2025-01-19 05:09:49,757 - ERROR - Error fetching land usage data for 10.7449, 92.5: module 'osmnx' has no attribute 'geometries_from_point'\n",
      "2025-01-19 05:09:49,758 - ERROR - Error fetching water coverage data for 10.7449, 92.5: module 'osmnx' has no attribute 'geometries_from_place'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define API keys and endpoints\n",
    "OPENWEATHER_API_KEY = \"9cdf050abe7b42592268c0bf78c0195a\"\n",
    "GBIF_API_BASE = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "\n",
    "# Add rate limiting parameters\n",
    "REQUEST_DELAY = 1  # Delay between API requests in seconds\n",
    "\n",
    "def fetch_climate_data(lat: float, lon: float) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch climate data using OpenWeatherMap API with improved precipitation handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # Improved precipitation handling\n",
    "        precipitation = 0\n",
    "        if \"rain\" in data:\n",
    "            precipitation = data[\"rain\"].get(\"1h\", 0) or data[\"rain\"].get(\"3h\", 0)\n",
    "        elif \"snow\" in data:\n",
    "            precipitation = data[\"snow\"].get(\"1h\", 0) or data[\"snow\"].get(\"3h\", 0)\n",
    "        \n",
    "        return {\n",
    "            \"temperature\": data[\"main\"][\"temp\"],\n",
    "            \"precipitation\": precipitation,\n",
    "            \"humidity\": data[\"main\"][\"humidity\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching climate data for {lat}, {lon}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def fetch_biodiversity_data(lat: float, lon: float) -> int:\n",
    "    \"\"\"\n",
    "    Fetch biodiversity data using GBIF API with improved species counting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"decimalLatitude\": f\"{lat-0.5},{lat+0.5}\",\n",
    "            \"decimalLongitude\": f\"{lon-0.5},{lon+0.5}\",\n",
    "            \"limit\": 300,  # Increased limit\n",
    "            \"hasCoordinate\": True,\n",
    "            \"hasGeospatialIssue\": False\n",
    "        }\n",
    "        response = requests.get(GBIF_API_BASE, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # Count unique species\n",
    "        species_set = set()\n",
    "        for record in data.get(\"results\", []):\n",
    "            if record.get(\"species\"):\n",
    "                species_set.add(record[\"species\"])\n",
    "        \n",
    "        return len(species_set)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching biodiversity data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def fetch_green_cover_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch green cover data with improved NDVI calculation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a larger area for analysis\n",
    "        dist = 1000  # increased from 500 to 1000 meters\n",
    "        tags = {\n",
    "            'landuse': ['forest', 'grass', 'park', 'meadow', 'recreation_ground'],\n",
    "            'natural': ['wood', 'grassland', 'heath']\n",
    "        }\n",
    "        \n",
    "        area = ox.geometries_from_point((lat, lon), tags=tags, dist=dist)\n",
    "        \n",
    "        if not area.empty:\n",
    "            total_area = np.pi * (dist ** 2)  # Total circular area\n",
    "            green_area = area.geometry.area.sum()\n",
    "            ndvi_proxy = green_area / total_area\n",
    "            return max(min(ndvi_proxy, 1), 0)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching green cover data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def fetch_land_usage_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch land usage data with improved urban density calculation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dist = 1000  # Analysis radius in meters\n",
    "        \n",
    "        # Get both buildings and roads\n",
    "        building_tags = {'building': True}\n",
    "        buildings = ox.geometries_from_point((lat, lon), tags=building_tags, dist=dist)\n",
    "        \n",
    "        graph = ox.graph_from_point((lat, lon), dist=dist, network_type='all')\n",
    "        \n",
    "        total_area = np.pi * (dist ** 2)\n",
    "        building_area = buildings.geometry.area.sum() if not buildings.empty else 0\n",
    "        road_length = sum(d['length'] for u, v, d in graph.edges(data=True))\n",
    "        \n",
    "        # Combine building coverage and road density for urban usage metric\n",
    "        urban_density = (building_area / total_area) + (road_length / (dist * 2 * np.pi))\n",
    "        return max(min(urban_density, 1), 0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching land usage data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def fetch_water_coverage_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch water coverage data with improved calculation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dist = 1000  # Analysis radius in meters\n",
    "        water_tags = {\n",
    "            'natural': ['water', 'wetland'],\n",
    "            'water': True,\n",
    "            'waterway': ['river', 'canal', 'stream']\n",
    "        }\n",
    "        \n",
    "        water_features = ox.geometries_from_place((lat, lon), tags=water_tags, dist=dist)\n",
    "        \n",
    "        if not water_features.empty:\n",
    "            total_area = np.pi * (dist ** 2)\n",
    "            water_area = water_features.geometry.area.sum()\n",
    "            water_coverage = water_area / total_area\n",
    "            return max(min(water_coverage, 1), 0)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching water coverage data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def generate_risk_score(\n",
    "    ndvi: float,\n",
    "    species_richness: int,\n",
    "    urban_land_usage: float,\n",
    "    water_coverage: float,\n",
    "    land_use_type: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate a risk score based on environmental factors.\n",
    "    \n",
    "    Args:\n",
    "        ndvi (float): Normalized Difference Vegetation Index\n",
    "        species_richness (int): Number of species in area\n",
    "        urban_land_usage (float): Urban density metric\n",
    "        water_coverage (float): Water coverage ratio\n",
    "        land_use_type (str): Type of land use\n",
    "        \n",
    "    Returns:\n",
    "        float: Risk score between 0 and 1\n",
    "    \"\"\"\n",
    "    land_use_weight = {\n",
    "        \"green-based use\": 0.1,\n",
    "        \"agricultural use\": 0.2,\n",
    "        \"urban home-type use\": 0.3,\n",
    "        \"commercial/industrial use\": 0.4\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        risk_score = (\n",
    "            (1 - ndvi) * 0.35 +\n",
    "            (species_richness / 100) * 0.25 +\n",
    "            (urban_land_usage / 100) * 0.25 +\n",
    "            (1 - water_coverage) * 0.15\n",
    "        )\n",
    "        \n",
    "        risk_score *= land_use_weight.get(land_use_type, 0.25)\n",
    "        return min(max(risk_score, 0), 1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating risk score: {str(e)}\")\n",
    "        return 0.5\n",
    "\n",
    "def create_dataset(\n",
    "    regions: List[Dict[str, float]],\n",
    "    land_use_types: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a labeled dataset across various regions with selected land use types.\n",
    "    \n",
    "    Args:\n",
    "        regions (list): List of dictionaries containing lat/lon coordinates\n",
    "        land_use_types (list): List of land use type strings\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataset containing environmental metrics and risk scores\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    total_regions = len(regions)\n",
    "    \n",
    "    for idx, region in enumerate(regions, 1):\n",
    "        try:\n",
    "            lat, lon = region[\"lat\"], region[\"lon\"]\n",
    "            logger.info(f\"Processing region {idx}/{total_regions}: {lat}, {lon}\")\n",
    "            \n",
    "            climate_data = fetch_climate_data(lat, lon)\n",
    "            biodiversity_data = fetch_biodiversity_data(lat, lon)\n",
    "            green_cover = fetch_green_cover_data(lat, lon)\n",
    "            land_usage = fetch_land_usage_data(lat, lon)\n",
    "            water_coverage = fetch_water_coverage_data(lat, lon)\n",
    "            \n",
    "            if climate_data:\n",
    "                for land_use_type in land_use_types:\n",
    "                    risk_score = generate_risk_score(\n",
    "                        green_cover,\n",
    "                        biodiversity_data,\n",
    "                        land_usage,\n",
    "                        water_coverage,\n",
    "                        land_use_type\n",
    "                    )\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"latitude\": lat,\n",
    "                        \"longitude\": lon,\n",
    "                        \"temperature\": climate_data[\"temperature\"],\n",
    "                        \"precipitation\": climate_data[\"precipitation\"],\n",
    "                        \"humidity\": climate_data[\"humidity\"],\n",
    "                        \"species_richness\": biodiversity_data,\n",
    "                        \"ndvi\": green_cover,\n",
    "                        \"urban_land_usage\": land_usage,\n",
    "                        \"water_coverage\": water_coverage,\n",
    "                        \"land_use_type\": land_use_type,\n",
    "                        \"risk_score\": risk_score,\n",
    "                        # \"timestamp\": datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing region {lat}, {lon}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data collection and analysis.\"\"\"\n",
    "    # Define regions (example coordinates for various landscapes)\n",
    "    regions = [\n",
    "        {\"lat\": 10.7449, \"lon\": 92.5000},  # New York City, USA\n",
    "        # {\"lat\": -33.8688, \"lon\": 151.2093},  # Sydney, Australia\n",
    "        # {\"lat\": 51.5074, \"lon\": -0.1278},   # London, UK\n",
    "        # {\"lat\": -1.286389, \"lon\": 36.817223},  # Nairobi, Kenya\n",
    "        # {\"lat\": 28.6139, \"lon\": 77.2090}    # New Delhi, India\n",
    "    ]\n",
    "\n",
    "    # Define possible land use types\n",
    "    land_use_types = [\n",
    "        \"green-based use\",\n",
    "        \"agricultural use\",\n",
    "        \"urban home-type use\",\n",
    "        \"commercial/industrial use\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Create the dataset\n",
    "        logger.info(\"Starting dataset creation...\")\n",
    "        dataset = create_dataset(regions, land_use_types)\n",
    "        \n",
    "        # Save to CSV\n",
    "        # output_filename = f\"urban_planning_risk_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        # dataset.to_csv(output_filename, index=False)\n",
    "        # logger.info(f\"Dataset successfully created and saved as '{output_filename}'\")\n",
    "        \n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        return None\n",
    "dataset = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d7e2ff-52e5-4e54-a99f-a205cda59186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 05:12:44,420 - INFO - Processing region 1/1: 10.7449, 92.5\n",
      "2025-01-19 05:12:51,256 - ERROR - Error fetching green cover data for 10.7449, 92.5: module 'osmnx' has no attribute 'geometries_from_point'\n",
      "2025-01-19 05:12:51,258 - ERROR - Error fetching land usage data for 10.7449, 92.5: module 'osmnx' has no attribute 'geometries_from_point'\n",
      "2025-01-19 05:12:51,259 - ERROR - Error fetching water coverage data for 10.7449, 92.5: module 'osmnx' has no attribute 'geometries_from_place'\n",
      "2025-01-19 05:12:51,445 - INFO - Processing region 1/1: 11.96, 75.92\n",
      "2025-01-19 05:13:03,652 - ERROR - Error in primary green cover fetch: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "import requests\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "from shapely.geometry import Polygon, MultiPolygon, LineString\n",
    "from shapely.ops import unary_union\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from typing import Optional\n",
    "import json\n",
    "import shap\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define API keys and endpoints\n",
    "OPENWEATHER_API_KEY = \"9cdf050abe7b42592268c0bf78c0195a\"\n",
    "GBIF_API_BASE = \"https://api.gbif.org/v1/occurrence/search\"\n",
    "\n",
    "# Add rate limiting parameters\n",
    "REQUEST_DELAY = 1  # Delay between API requests in seconds\n",
    "\n",
    "def fetch_climate_data(lat: float, lon: float) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch climate data using OpenWeatherMap API with improved precipitation handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # Improved precipitation handling\n",
    "        precipitation = 0\n",
    "        if \"rain\" in data:\n",
    "            precipitation = data[\"rain\"].get(\"1h\", 0) or data[\"rain\"].get(\"3h\", 0)\n",
    "        elif \"snow\" in data:\n",
    "            precipitation = data[\"snow\"].get(\"1h\", 0) or data[\"snow\"].get(\"3h\", 0)\n",
    "        \n",
    "        return {\n",
    "            \"temperature\": data[\"main\"][\"temp\"],\n",
    "            \"precipitation\": precipitation,\n",
    "            \"humidity\": data[\"main\"][\"humidity\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching climate data for {lat}, {lon}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def fetch_biodiversity_data(lat: float, lon: float) -> int:\n",
    "    \"\"\"\n",
    "    Fetch biodiversity data using GBIF API with improved species counting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"decimalLatitude\": f\"{lat-0.5},{lat+0.5}\",\n",
    "            \"decimalLongitude\": f\"{lon-0.5},{lon+0.5}\",\n",
    "            \"limit\": 300,  # Increased limit\n",
    "            \"hasCoordinate\": True,\n",
    "            \"hasGeospatialIssue\": False\n",
    "        }\n",
    "        response = requests.get(GBIF_API_BASE, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # Count unique species\n",
    "        species_set = set()\n",
    "        for record in data.get(\"results\", []):\n",
    "            if record.get(\"species\"):\n",
    "                species_set.add(record[\"species\"])\n",
    "        \n",
    "        return len(species_set)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching biodiversity data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def fetch_green_cover_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch green cover data with improved NDVI calculation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a larger area for analysis\n",
    "        dist = 1000  # increased from 500 to 1000 meters\n",
    "        tags = {\n",
    "            'landuse': ['forest', 'grass', 'park', 'meadow', 'recreation_ground'],\n",
    "            'natural': ['wood', 'grassland', 'heath']\n",
    "        }\n",
    "        \n",
    "        area = ox.geometries_from_point((lat, lon), tags=tags, dist=dist)\n",
    "        \n",
    "        if not area.empty:\n",
    "            total_area = np.pi * (dist ** 2)  # Total circular area\n",
    "            green_area = area.geometry.area.sum()\n",
    "            ndvi_proxy = green_area / total_area\n",
    "            return max(min(ndvi_proxy, 1), 0)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching green cover data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def fetch_land_usage_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch land usage data with improved urban density calculation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dist = 1000  # Analysis radius in meters\n",
    "        \n",
    "        # Get both buildings and roads\n",
    "        building_tags = {'building': True}\n",
    "        buildings = ox.geometries_from_point((lat, lon), tags=building_tags, dist=dist)\n",
    "        \n",
    "        graph = ox.graph_from_point((lat, lon), dist=dist, network_type='all')\n",
    "        \n",
    "        total_area = np.pi * (dist ** 2)\n",
    "        building_area = buildings.geometry.area.sum() if not buildings.empty else 0\n",
    "        road_length = sum(d['length'] for u, v, d in graph.edges(data=True))\n",
    "        \n",
    "        # Combine building coverage and road density for urban usage metric\n",
    "        urban_density = (building_area / total_area) + (road_length / (dist * 2 * np.pi))\n",
    "        return max(min(urban_density, 1), 0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching land usage data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def fetch_water_coverage_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch water coverage data with improved calculation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dist = 1000  # Analysis radius in meters\n",
    "        water_tags = {\n",
    "            'natural': ['water', 'wetland'],\n",
    "            'water': True,\n",
    "            'waterway': ['river', 'canal', 'stream']\n",
    "        }\n",
    "        \n",
    "        water_features = ox.geometries_from_place((lat, lon), tags=water_tags, dist=dist)\n",
    "        \n",
    "        if not water_features.empty:\n",
    "            total_area = np.pi * (dist ** 2)\n",
    "            water_area = water_features.geometry.area.sum()\n",
    "            water_coverage = water_area / total_area\n",
    "            return max(min(water_coverage, 1), 0)\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching water coverage data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def generate_risk_score(\n",
    "    ndvi: float,\n",
    "    species_richness: int,\n",
    "    urban_land_usage: float,\n",
    "    water_coverage: float,\n",
    "    land_use_type: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate a risk score based on environmental factors.\n",
    "    \n",
    "    Args:\n",
    "        ndvi (float): Normalized Difference Vegetation Index\n",
    "        species_richness (int): Number of species in area\n",
    "        urban_land_usage (float): Urban density metric\n",
    "        water_coverage (float): Water coverage ratio\n",
    "        land_use_type (str): Type of land use\n",
    "        \n",
    "    Returns:\n",
    "        float: Risk score between 0 and 1\n",
    "    \"\"\"\n",
    "    land_use_weight = {\n",
    "        \"green-based use\": 0.1,\n",
    "        \"agricultural use\": 0.2,\n",
    "        \"urban home-type use\": 0.3,\n",
    "        \"commercial/industrial use\": 0.4\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        risk_score = (\n",
    "            (1 - ndvi) * 0.35 +\n",
    "            (species_richness / 100) * 0.25 +\n",
    "            (urban_land_usage / 100) * 0.25 +\n",
    "            (1 - water_coverage) * 0.15\n",
    "        )\n",
    "        \n",
    "        risk_score *= land_use_weight.get(land_use_type, 0.25)\n",
    "        return min(max(risk_score, 0), 1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating risk score: {str(e)}\")\n",
    "        return 0.5\n",
    "\n",
    "def create_dataset(\n",
    "    regions: List[Dict[str, float]],\n",
    "    land_use_types: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a labeled dataset across various regions with selected land use types.\n",
    "    \n",
    "    Args:\n",
    "        regions (list): List of dictionaries containing lat/lon coordinates\n",
    "        land_use_types (list): List of land use type strings\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataset containing environmental metrics and risk scores\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    total_regions = len(regions)\n",
    "    \n",
    "    for idx, region in enumerate(regions, 1):\n",
    "        try:\n",
    "            lat, lon = region[\"lat\"], region[\"lon\"]\n",
    "            logger.info(f\"Processing region {idx}/{total_regions}: {lat}, {lon}\")\n",
    "            \n",
    "            climate_data = fetch_climate_data(lat, lon)\n",
    "            biodiversity_data = fetch_biodiversity_data(lat, lon)\n",
    "            green_cover = fetch_green_cover_data(lat, lon)\n",
    "            land_usage = fetch_land_usage_data(lat, lon)\n",
    "            water_coverage = fetch_water_coverage_data(lat, lon)\n",
    "            \n",
    "            if climate_data:\n",
    "                for land_use_type in land_use_types:\n",
    "                    risk_score = generate_risk_score(\n",
    "                        green_cover,\n",
    "                        biodiversity_data,\n",
    "                        land_usage,\n",
    "                        water_coverage,\n",
    "                        land_use_type\n",
    "                    )\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"latitude\": lat,\n",
    "                        \"longitude\": lon,\n",
    "                        \"temperature\": climate_data[\"temperature\"],\n",
    "                        \"precipitation\": climate_data[\"precipitation\"],\n",
    "                        \"humidity\": climate_data[\"humidity\"],\n",
    "                        \"species_richness\": biodiversity_data,\n",
    "                        \"ndvi\": green_cover,\n",
    "                        \"urban_land_usage\": land_usage,\n",
    "                        \"water_coverage\": water_coverage,\n",
    "                        \"land_use_type\": land_use_type,\n",
    "                        \"risk_score\": risk_score,\n",
    "                        # \"timestamp\": datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing region {lat}, {lon}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "regions = [\n",
    "        {\"lat\": 10.7449, \"lon\": 92.5000},  # New York City, USA\n",
    "        # {\"lat\": -33.8688, \"lon\": 151.2093},  # Sydney, Australia\n",
    "        # {\"lat\": 51.5074, \"lon\": -0.1278},   # London, UK\n",
    "        # {\"lat\": -1.286389, \"lon\": 36.817223},  # Nairobi, Kenya\n",
    "        # {\"lat\": 28.6139, \"lon\": 77.2090}    # New Delhi, India\n",
    "    ]\n",
    "\n",
    "    # Define possible land use types\n",
    "land_use_types = [\n",
    "        \"green-based use\",\n",
    "        \"agricultural use\",\n",
    "        \"urban home-type use\",\n",
    "        \"commercial/industrial use\"\n",
    "    ]\n",
    "\n",
    "dataset = create_dataset(regions, land_use_types)\n",
    "\n",
    "#change this shit\n",
    "cache = {'latitude':11.96,\n",
    "        'longitude':75.92,\n",
    "        'use_case_type':'Agricultural'}\n",
    "\n",
    "with open('models2.pkl', 'rb') as f:\n",
    "    loaded_models = pickle.load(f)\n",
    "\n",
    "with open('ensemble_weights2.pkl', 'rb') as f:\n",
    "    loaded_weights = pickle.load(f)\n",
    "    \n",
    "with open('preprocessors2.pkl', 'rb') as f:\n",
    "    loaded_encoders, loaded_scaler = pickle.load(f)\n",
    "\n",
    "latitude = cache['latitude']\n",
    "longitude = cache['longitude']\n",
    "use_case_type = cache['use_case_type']\n",
    "\n",
    "def fetch_climate_data(lat: float, lon: float) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch climate data using OpenWeatherMap API with 5-day precipitation forecast.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get current weather\n",
    "        current_url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric\"\n",
    "        current_response = requests.get(current_url)\n",
    "        current_response.raise_for_status()\n",
    "        current_data = current_response.json()\n",
    "        \n",
    "        # Get 5 day forecast with 3-hour steps\n",
    "        forecast_url = f\"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric\"\n",
    "        forecast_response = requests.get(forecast_url)\n",
    "        forecast_response.raise_for_status()\n",
    "        forecast_data = forecast_response.json()\n",
    "        \n",
    "        # Calculate average precipitation from forecast\n",
    "        total_precipitation = 0\n",
    "        count = 0\n",
    "        \n",
    "        for item in forecast_data.get('list', []):\n",
    "            # Get precipitation (rain or snow)\n",
    "            rain_amount = item.get('rain', {}).get('3h', 0)\n",
    "            snow_amount = item.get('snow', {}).get('3h', 0)\n",
    "            total_precipitation += rain_amount + snow_amount\n",
    "            count += 1\n",
    "        \n",
    "        # Convert 3-hourly precipitation to daily average\n",
    "        avg_daily_precipitation = (total_precipitation / count) * 8 if count > 0 else 0\n",
    "        # Estimate monthly precipitation (multiply by 30 days)\n",
    "        estimated_monthly_precipitation = avg_daily_precipitation * 30\n",
    "        \n",
    "        return {\n",
    "            \"temperature\": current_data[\"main\"][\"temp\"],\n",
    "            \"precipitation\": estimated_monthly_precipitation,\n",
    "            \"humidity\": current_data[\"main\"][\"humidity\"],\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching climate data for {lat}, {lon}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "def _get_overpass_data(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Helper function to fetch data from Overpass API.\n",
    "    \"\"\"\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    try:\n",
    "        response = requests.post(overpass_url, data={'data': query})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Overpass API error: {str(e)}\")\n",
    "        return {'elements': []}\n",
    "\n",
    "def fetch_land_usage_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch land usage data using Overpass API.\n",
    "    Returns urban density index between 0 and 1.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate bounding box (1km radius)\n",
    "        radius = 1000  # meters\n",
    "        deg_radius = radius / 111320  # Convert meters to degrees (approximate)\n",
    "        \n",
    "        # Overpass query for buildings and roads\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:25];\n",
    "        (\n",
    "          way[\"building\"](around:{radius},{lat},{lon});\n",
    "          way[\"highway\"](around:{radius},{lat},{lon});\n",
    "        );\n",
    "        out body geom;\n",
    "        \"\"\"\n",
    "        \n",
    "        data = _get_overpass_data(query)\n",
    "        \n",
    "        if not data['elements']:\n",
    "            # Fallback to secondary API\n",
    "            return _fetch_land_usage_fallback(lat, lon)\n",
    "        \n",
    "        # Calculate areas and lengths\n",
    "        total_area = np.pi * (radius ** 2)  # Total circular area in square meters\n",
    "        building_area = 0\n",
    "        road_length = 0\n",
    "        \n",
    "        for element in data['elements']:\n",
    "            if 'geometry' in element:\n",
    "                coords = [(p['lon'], p['lat']) for p in element['geometry']]\n",
    "                if element.get('tags', {}).get('building'):\n",
    "                    # Calculate building area\n",
    "                    if len(coords) >= 3:\n",
    "                        try:\n",
    "                            polygon = Polygon(coords)\n",
    "                            building_area += polygon.area * 111320 * 111320  # Convert to square meters\n",
    "                        except:\n",
    "                            continue\n",
    "                elif element.get('tags', {}).get('highway'):\n",
    "                    # Calculate road length\n",
    "                    if len(coords) >= 2:\n",
    "                        try:\n",
    "                            line = LineString(coords)\n",
    "                            road_length += line.length * 111320  # Convert to meters\n",
    "                        except:\n",
    "                            continue\n",
    "        \n",
    "        # Calculate urban density\n",
    "        building_density = min(building_area / total_area, 0.7)  # Cap at 70%\n",
    "        road_density = min(road_length / (radius * 2 * np.pi), 0.3)  # Cap at 30%\n",
    "        \n",
    "        urban_density = building_density + road_density\n",
    "        return min(max(urban_density, 0), 1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error in primary land usage calculation: {str(e)}\")\n",
    "        return _fetch_land_usage_fallback(lat, lon)\n",
    "\n",
    "def _fetch_land_usage_fallback(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fallback method using OpenStreetMap Nominatim API for land use data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use Nominatim API to get area details\n",
    "        nominatim_url = f\"https://nominatim.openstreetmap.org/reverse?lat={lat}&lon={lon}&format=json&zoom=14\"\n",
    "        headers = {'User-Agent': 'Urban Density Calculator 1.0'}\n",
    "        \n",
    "        response = requests.get(nominatim_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Analyze address components and category\n",
    "        address = data.get('address', {})\n",
    "        category = data.get('category', '')\n",
    "        \n",
    "        # Calculate urban density based on location type\n",
    "        if any(k in address for k in ['city', 'town', 'suburb']):\n",
    "            return 0.8  # Urban area\n",
    "        elif 'village' in address:\n",
    "            return 0.4  # Rural settlement\n",
    "        elif any(k in address for k in ['industrial', 'commercial']):\n",
    "            return 0.9  # Industrial/commercial area\n",
    "        elif any(k in address for k in ['forest', 'park', 'nature_reserve']):\n",
    "            return 0.1  # Natural area\n",
    "        else:\n",
    "            return 0.5  # Default semi-urban\n",
    "            \n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error in fallback land usage calculation: {str(e)}\")\n",
    "        return 0.5  # Default value\n",
    "\n",
    "def fetch_water_coverage_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch water coverage data using Overpass API.\n",
    "    Returns water coverage ratio between 0 and 1.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate bounding box (1km radius)\n",
    "        radius = 1000  # meters\n",
    "        \n",
    "        # Overpass query for water features\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:25];\n",
    "        (\n",
    "          way[\"natural\"=\"water\"](around:{radius},{lat},{lon});\n",
    "          way[\"waterway\"](around:{radius},{lat},{lon});\n",
    "          way[\"water\"](around:{radius},{lat},{lon});\n",
    "          way[\"natural\"=\"wetland\"](around:{radius},{lat},{lon});\n",
    "        );\n",
    "        out body geom;\n",
    "        \"\"\"\n",
    "        \n",
    "        data = _get_overpass_data(query)\n",
    "        \n",
    "        if not data['elements']:\n",
    "            # Fallback to secondary API\n",
    "            return _fetch_water_coverage_fallback(lat, lon)\n",
    "        \n",
    "        # Calculate areas\n",
    "        total_area = np.pi * (radius ** 2)  # Total circular area in square meters\n",
    "        water_area = 0\n",
    "        \n",
    "        for element in data['elements']:\n",
    "            if 'geometry' in element:\n",
    "                coords = [(p['lon'], p['lat']) for p in element['geometry']]\n",
    "                if len(coords) >= 3:\n",
    "                    try:\n",
    "                        polygon = Polygon(coords)\n",
    "                        water_area += polygon.area * 111320 * 111320  # Convert to square meters\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        water_coverage = water_area / total_area\n",
    "        return min(max(water_coverage, 0), 1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error in primary water coverage calculation: {str(e)}\")\n",
    "        return _fetch_water_coverage_fallback(lat, lon)\n",
    "\n",
    "def _fetch_water_coverage_fallback(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fallback method using OpenMeteo API for water proximity data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use OpenMeteo API for water-related data\n",
    "        url = f\"https://marine-api.open-meteo.com/v1/marine?latitude={lat}&longitude={lon}&daily=wave_height\"\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'daily' in data and 'wave_height' in data['daily']:\n",
    "            # If wave height data is available, location is near water\n",
    "            return min(max(np.mean(data['daily']['wave_height']) / 2, 0), 1)\n",
    "            \n",
    "        # If no marine data, check for inland water bodies using Nominatim\n",
    "        nominatim_url = f\"https://nominatim.openstreetmap.org/reverse?lat={lat}&lon={lon}&format=json&zoom=14\"\n",
    "        headers = {'User-Agent': 'Water Coverage Calculator 1.0'}\n",
    "        \n",
    "        response = requests.get(nominatim_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check location type\n",
    "        if any(water_type in str(data).lower() for water_type in ['lake', 'river', 'sea', 'ocean', 'bay', 'wetland']):\n",
    "            return 0.7  # Significant water presence\n",
    "        return 0.1  # Minimal water presence\n",
    "            \n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error in fallback water coverage calculation: {str(e)}\")\n",
    "        return 0.1  # Default low water coverage\n",
    "\n",
    "def fetch_green_cover_data(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fetch green cover data using Copernicus Global Land Service API.\n",
    "    Uses NDVI (Normalized Difference Vegetation Index) data.\n",
    "    \n",
    "    Args:\n",
    "        lat (float): Latitude\n",
    "        lon (float): Longitude\n",
    "    \n",
    "    Returns:\n",
    "        float: NDVI value between 0 and 1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using Copernicus Global Land Service API\n",
    "        base_url = \"https://land.copernicus.vgt.vito.be/REST/TimeSeries/1.0/extract\"\n",
    "        \n",
    "        # Current date and one month ago\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=30)\n",
    "        \n",
    "        params = {\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'startdate': start_date.strftime('%Y-%m-%d'),\n",
    "            'enddate': end_date.strftime('%Y-%m-%d'),\n",
    "            'collection': 'NDVI_V2',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            # Fallback to alternative API: OpenMeteo\n",
    "            return _fetch_green_cover_fallback(lat, lon)\n",
    "            \n",
    "        data = response.json()\n",
    "        ndvi_values = [item['NDVI'] for item in data['results'] if 'NDVI' in item]\n",
    "        \n",
    "        if ndvi_values:\n",
    "            # NDVI values are typically between -1 and 1\n",
    "            # Normalize to 0-1 range\n",
    "            avg_ndvi = np.mean(ndvi_values)\n",
    "            normalized_ndvi = (avg_ndvi + 1) / 2\n",
    "            return max(min(normalized_ndvi, 1), 0)\n",
    "            \n",
    "        return _fetch_green_cover_fallback(lat, lon)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in primary green cover fetch: {str(e)}\")\n",
    "        return _fetch_green_cover_fallback(lat, lon)\n",
    "\n",
    "def _fetch_green_cover_fallback(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Fallback method using OpenMeteo API for vegetation data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # OpenMeteo API for soil and vegetation data\n",
    "        url = (f\"https://api.open-meteo.com/v1/forecast?\"\n",
    "               f\"latitude={lat}&longitude={lon}\"\n",
    "               f\"&hourly=soil_moisture_0_1cm,soil_moisture_1_3cm\"\n",
    "               f\"&daily=et0_fao_evapotranspiration\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Calculate green cover proxy using soil moisture and evapotranspiration\n",
    "        soil_moisture = np.mean(data['hourly']['soil_moisture_0_1cm'][:24])  # First 24 hours\n",
    "        evapotranspiration = data['daily']['et0_fao_evapotranspiration'][0]  # First day\n",
    "        \n",
    "        # Combine metrics to estimate vegetation cover\n",
    "        # Normalize values based on typical ranges\n",
    "        soil_moisture_norm = min(soil_moisture / 50, 1)  # Typical range 0-50\n",
    "        evapotrans_norm = min(evapotranspiration / 10, 1)  # Typical range 0-10\n",
    "        \n",
    "        # Weight the factors\n",
    "        green_cover = (soil_moisture_norm * 0.6 + evapotrans_norm * 0.4)\n",
    "        \n",
    "        return max(min(green_cover, 1), 0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error in fallback green cover fetch: {str(e)}\")\n",
    "        return _fetch_green_cover_last_resort(lat, lon)\n",
    "\n",
    "def _fetch_green_cover_last_resort(lat: float, lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Last resort method using NASA POWER API for vegetation-related data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NASA POWER API\n",
    "        base_url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "        \n",
    "        params = {\n",
    "            'parameters': 'T2M,PRECTOT,RH2M',  # Temperature, precipitation, humidity\n",
    "            'community': 'AG',\n",
    "            'longitude': lon,\n",
    "            'latitude': lat,\n",
    "            'start': datetime.now().strftime('%Y%m%d'),\n",
    "            'end': datetime.now().strftime('%Y%m%d'),\n",
    "            'format': 'JSON'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract relevant parameters\n",
    "        temp = float(data['properties']['parameter']['T2M'][datetime.now().strftime('%Y%m%d')])\n",
    "        precip = float(data['properties']['parameter']['PRECTOT'][datetime.now().strftime('%Y%m%d')])\n",
    "        humidity = float(data['properties']['parameter']['RH2M'][datetime.now().strftime('%Y%m%d')])\n",
    "        \n",
    "        # Create a simple vegetation index based on environmental conditions\n",
    "        # This is a rough approximation based on typical conditions favorable for vegetation\n",
    "        temp_factor = max(0, min(1 - abs(temp - 20) / 30, 1))  # Optimal temp around 20°C\n",
    "        precip_factor = min(precip / 10, 1)  # Normalize precipitation (0-10mm)\n",
    "        humidity_factor = humidity / 100  # Humidity is already 0-100\n",
    "        \n",
    "        # Combine factors with weights\n",
    "        green_cover = (temp_factor * 0.3 + precip_factor * 0.4 + humidity_factor * 0.3)\n",
    "        \n",
    "        return max(min(green_cover, 1), 0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error in last resort green cover fetch: {str(e)}\")\n",
    "        # Return a reasonable default based on global averages\n",
    "        return 0.3  # Global average vegetation cover is roughly 30%\n",
    "\n",
    "def fetch_biodiversity_data(lat: float, lon: float) -> int:\n",
    "    \"\"\"\n",
    "    Fetch biodiversity data using GBIF API with improved species counting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"decimalLatitude\": f\"{lat-0.5},{lat+0.5}\",\n",
    "            \"decimalLongitude\": f\"{lon-0.5},{lon+0.5}\",\n",
    "            \"limit\": 300,  # Increased limit\n",
    "            \"hasCoordinate\": True,\n",
    "            \"hasGeospatialIssue\": False\n",
    "        }\n",
    "        response = requests.get(GBIF_API_BASE, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # Count unique species\n",
    "        species_set = set()\n",
    "        for record in data.get(\"results\", []):\n",
    "            if record.get(\"species\"):\n",
    "                species_set.add(record[\"species\"])\n",
    "        \n",
    "        return len(species_set)\n",
    "    except Exception as e:\n",
    "        # logger.error(f\"Error fetching biodiversity data for {lat}, {lon}: {str(e)}\")\n",
    "        return 0   \n",
    "\n",
    "def create_dataset(\n",
    "    cache):\n",
    "    \"\"\"\n",
    "    Create a labeled dataset across various regions with selected land use types.\n",
    "    \n",
    "    Args:\n",
    "        regions (list): List of dictionaries containing lat/lon coordinates\n",
    "        land_use_types (list): List of land use type strings\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataset containing environmental metrics and risk scores\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    regions = [{\"lat\": cache['latitude'], \"lon\": cache['longitude']}]\n",
    "    land_use_types = [cache['use_case_type']]\n",
    "    total_regions = len(regions)\n",
    "    \n",
    "    for idx, region in enumerate(regions, 1):\n",
    "        try:\n",
    "            lat, lon = region[\"lat\"], region[\"lon\"]\n",
    "            logger.info(f\"Processing region {idx}/{total_regions}: {lat}, {lon}\")\n",
    "            \n",
    "            climate_data = fetch_climate_data(lat, lon)\n",
    "            biodiversity_data = fetch_biodiversity_data(lat, lon)\n",
    "            green_cover = fetch_green_cover_data(lat, lon)\n",
    "            land_usage = fetch_land_usage_data(lat, lon)\n",
    "            water_coverage = fetch_water_coverage_data(lat, lon)\n",
    "            \n",
    "            if climate_data:\n",
    "                for land_use_type in land_use_types:\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"latitude\": lat,\n",
    "                        \"longitude\": lon,\n",
    "                        \"temperature\": climate_data[\"temperature\"],\n",
    "                        \"precipitation\": climate_data[\"precipitation\"],\n",
    "                        \"humidity\": climate_data[\"humidity\"],\n",
    "                        \"species_richness\": biodiversity_data,\n",
    "                        \"ndvi\": green_cover,\n",
    "                        \"urban_land_usage\": land_usage,\n",
    "                        \"water_coverage\": water_coverage,\n",
    "                        \"land_use_type\": land_use_type,\n",
    "                        # \"timestamp\": datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error processing region {lat}, {lon}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def preprocess_inference_data(input_data, label_encoders, scaler):\n",
    "    \"\"\"\n",
    "    Preprocess input data for inference.\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): New input data for prediction.\n",
    "        label_encoders (dict): Dictionary of fitted LabelEncoders for categorical features.\n",
    "        scaler (StandardScaler): Fitted StandardScaler for numerical features.\n",
    "        categorical_columns (list): List of categorical feature names.\n",
    "        numerical_columns (list): List of numerical feature names.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed input data.\n",
    "    \"\"\"\n",
    "    data = input_data.copy()\n",
    "\n",
    "    categorical_columns = ['land_use_type']\n",
    "    numerical_columns = ['latitude', 'longitude', 'temperature', 'precipitation', \n",
    "                         'ndvi', 'urban_land_usage', 'water_coverage','humidity', 'species_richness']\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        if col in data.columns and col in label_encoders:\n",
    "            data[col] = label_encoders[col].transform(data[col])\n",
    "    \n",
    "    # Scale numerical features\n",
    "    if set(numerical_columns).issubset(data.columns):\n",
    "        data[numerical_columns] = scaler.transform(data[numerical_columns])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def predict(input_data, models, weights, label_encoders, scaler):\n",
    "    \"\"\"\n",
    "    Perform inference on input data.\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): New input data for prediction.\n",
    "        models (dict): Dictionary of trained models.\n",
    "        weights (dict): Dictionary of model weights for ensemble.\n",
    "        label_encoders (dict): Fitted label encoders for categorical features.\n",
    "        scaler (StandardScaler): Fitted scaler for numerical features.\n",
    "        categorical_columns (list): List of categorical feature names.\n",
    "        numerical_columns (list): List of numerical feature names.\n",
    "    \n",
    "    Returns:\n",
    "        float: Final ensemble prediction for `risk_score`.\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_inference_data(input_data, label_encoders, scaler)\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    predictions = {\n",
    "        name: model.predict(processed_data) for name, model in models.items()\n",
    "    }\n",
    "    \n",
    "    # Compute weighted ensemble prediction\n",
    "    ensemble_prediction = sum(predictions[model] * weight for model, weight in weights.items())\n",
    "    return ensemble_prediction\n",
    "\n",
    "def calculate_shap_values(input_data: pd.DataFrame, models: dict, weights: dict, label_encoders: dict, scaler: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate SHAP values for the ensemble model predictions.\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): Input data for prediction\n",
    "        models (dict): Dictionary of trained models\n",
    "        weights (dict): Dictionary of model weights for ensemble\n",
    "        label_encoders (dict): Fitted label encoders for categorical features\n",
    "        scaler (dict): Fitted scaler for numerical features\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing SHAP values for each feature\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_inference_data(input_data, label_encoders, scaler)\n",
    "    \n",
    "    # Initialize SHAP values array\n",
    "    final_shap_values = np.zeros(processed_data.shape[1])\n",
    "    \n",
    "    # Calculate SHAP values for each model and apply ensemble weights\n",
    "    for model_name, model in models.items():\n",
    "        # Create explainer based on model type\n",
    "        if isinstance(model, LGBMRegressor):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        elif isinstance(model, XGBRegressor):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        elif isinstance(model, CatBoostRegressor):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Calculate SHAP values for current model\n",
    "        shap_values = explainer.shap_values(processed_data)\n",
    "        \n",
    "        # If shap_values is a list (happens with some models), take the first element\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "            \n",
    "        # Apply model weight to SHAP values\n",
    "        weighted_shap = shap_values * weights[model_name]\n",
    "        \n",
    "        # Add to final SHAP values\n",
    "        final_shap_values += weighted_shap[0]  # Take first row as we're only predicting for one instance\n",
    "    \n",
    "    # Create dictionary with feature names and their SHAP values\n",
    "    feature_names = ['latitude', 'longitude', 'temperature', 'precipitation', \n",
    "                    'humidity', 'species_richness', 'ndvi', 'urban_land_usage', \n",
    "                    'water_coverage', 'land_use_type']\n",
    "    \n",
    "    shap_dict = {\n",
    "        feature: float(shap_value)  # Convert numpy float to Python float for JSON serialization\n",
    "        for feature, shap_value in zip(feature_names, final_shap_values)\n",
    "    }\n",
    "    \n",
    "    # Sort dictionary by absolute SHAP values to show features in order of importance\n",
    "    shap_dict = dict(sorted(shap_dict.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "    \n",
    "    return {\n",
    "        \"shap_values\": shap_dict,\n",
    "        \"explanation\": {\n",
    "            \"positive_impact\": [k for k, v in shap_dict.items() if v > 0],\n",
    "            \"negative_impact\": [k for k, v in shap_dict.items() if v < 0],\n",
    "            \"most_influential_features\": list(shap_dict.keys())[:3]\n",
    "        }\n",
    "    }\n",
    "input_data = create_dataset(cache)\n",
    "risk_score_prediction = predict(\n",
    "        input_data=input_data,\n",
    "        models=loaded_models,\n",
    "        weights=loaded_weights,\n",
    "        label_encoders=loaded_encoders,\n",
    "        scaler=loaded_scaler\n",
    "    )\n",
    "\n",
    "shap_analysis = calculate_shap_values(\n",
    "    input_data=input_data,\n",
    "    models=loaded_models,\n",
    "    weights=loaded_weights,\n",
    "    label_encoders=loaded_encoders,\n",
    "    scaler=loaded_scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bcc7160b-3533-4a29-9fba-ab3b01c5d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 05:02:32,069 - INFO - Request URL: 'https://models.inference.ai.azure.com/chat/completions?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '2370'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': '7d2e29b0-d5f4-11ef-8995-0653471ccb1d'\n",
      "    'api-key': 'REDACTED'\n",
      "    'User-Agent': 'azsdk-python-ai-inference/1.0.0b6 Python/3.12.4 (macOS-15.2-arm64-arm-64bit)'\n",
      "    'Authorization': 'REDACTED'\n",
      "A body is sent with the request\n",
      "2025-01-19 05:02:38,595 - INFO - Response status: 200\n",
      "Response headers:\n",
      "    'Date': 'Sat, 18 Jan 2025 23:32:38 GMT'\n",
      "    'Content-Type': 'application/json'\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Connection': 'keep-alive'\n",
      "    'Vary': 'REDACTED'\n",
      "    'x-ms-client-request-id': '7d2e29b0-d5f4-11ef-8995-0653471ccb1d'\n",
      "    'request-context': 'REDACTED'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'strict-transport-security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-rai-invoked': 'REDACTED'\n",
      "    'x-request-id': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'x-ratelimit-remaining-requests': 'REDACTED'\n",
      "    'x-ratelimit-remaining-tokens': 'REDACTED'\n",
      "    'azureml-model-session': 'REDACTED'\n",
      "    'x-aml-cluster': 'REDACTED'\n",
      "    'x-request-time': 'REDACTED'\n",
      "    'Content-Encoding': 'REDACTED'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Risk Analysis Summary\n",
      "\n",
      "The biodiversity risk score of 0.39 (Medium) for the specified agricultural location indicates a moderate concern for local biodiversity loss. This score is influenced by several interacting factors, including land use changes, the presence of invasive species, and habitat fragmentation due to agricultural expansion. The unique ecological characteristics of the region are likely affected by agricultural practices that may prioritize productivity over conserving natural habitats.\n",
      "\n",
      "### SHAP Impact Insights\n",
      "\n",
      "1. **Land Use Change (Impact: 0.15)**\n",
      "   - Significant conversions of natural habitats into agricultural zones have led to habitat degradation. This feature indicates a direct link between agricultural intensification and biodiversity decline.\n",
      "  \n",
      "2. **Invasive Species (Impact: 0.12)**\n",
      "   - The presence and spread of non-native species that outcompete native flora and fauna contribute to biodiversity loss. Their prevalence indicates that management and control measures are crucial.\n",
      "   \n",
      "3. **Habitat Fragmentation (Impact: 0.10)**\n",
      "   - Fragmented habitats limit species movement and gene flow, which are essential for maintaining healthy populations. This factor highlights the need for connectivity enhancement between isolated habitats.\n",
      "   \n",
      "**Interactions**: The interaction between land use change and invasive species presence magnifies the risk. As natural habitats diminish due to agricultural expansion, the likelihood of invasive species establishing themselves increases, further exacerbating biodiversity loss.\n",
      "\n",
      "### Actionable Recommendations\n",
      "\n",
      "1. **Urban Planning Solutions:**\n",
      "   - Design biodiversity-friendly agricultural zones that incorporate buffer areas around natural habitats.\n",
      "   - Create mixed-use areas that integrate urban infrastructure with green spaces to foster biodiversity.\n",
      "   - Implement regulations that create ecological setbacks in areas near sensitive ecosystems.\n",
      "\n",
      "2. **Green Corridor Placement:**\n",
      "   - Establish green corridors connecting fragmented habitats that allow for species movement. These corridors should be placed strategically to link areas of high biodiversity value.\n",
      "   - Identify under-utilized agricultural land or edges where green strip planting can occur to enhance connectivity without impeding agricultural productivity.\n",
      "\n",
      "3. **Native Vegetation Preservation:**\n",
      "   - Encourage the planting and preservation of native vegetation on agricultural margins to serve as habitat for local wildlife and promote ecological resilience.\n",
      "   - Promote biodiversity-friendly infrastructure such as pollinator gardens, hedgerows, and native grass strips along field edges.\n",
      "\n",
      "### Implementation and Monitoring\n",
      "\n",
      "1. **Prioritize Actions Based on SHAP Impacts:**\n",
      "   - Focus first on land use change as it has the most significant impact. Also prioritize managing invasive species, followed by efforts to mitigate habitat fragmentation.\n",
      "   \n",
      "2. **Phased Strategies:**\n",
      "   - **Phase 1:** Conduct a comprehensive assessment of current land use and invasive species presence; engage stakeholders.\n",
      "   - **Phase 2:** Develop and implement habitat restoration projects and green corridor systems.\n",
      "   - **Phase 3:** Monitor the effects of implemented changes on local biodiversity and adjust management strategies accordingly.\n",
      "\n",
      "3. **Monitoring Guidelines:**\n",
      "   - Establish key biodiversity indicators (e.g., species diversity, abundance of native species) to measure the effectiveness of implemented strategies.\n",
      "   - Create a feedback loop with regular assessments (every 6-12 months) of habitat quality and biodiversity health to adaptively manage ongoing activities.\n",
      "\n",
      "By grounding these recommendations in SHAP-based evidence, the focus remains on strategically enhancing urban planning and vegetation management efforts that respect the ecological integrity of the area while promoting agricultural productivity.\n"
     ]
    }
   ],
   "source": [
    "# !pip install azure-ai-inference\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def generate(system_prompt,ques):\n",
    "    client = ChatCompletionsClient(\n",
    "        endpoint=\"https://models.inference.ai.azure.com\",\n",
    "        credential=AzureKeyCredential('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'),\n",
    "    )\n",
    "    \n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=system_prompt),\n",
    "            UserMessage(content=ques),\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=1,\n",
    "        max_tokens=4096,\n",
    "        top_p=1\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "SHAP = shap_analysis\n",
    "risk_score = risk_score_prediction\n",
    "\n",
    "def get_risk_level(score):\n",
    "        if score <= 0.3:\n",
    "            return \"Low\"\n",
    "        elif score <= 0.6:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    \n",
    "def get_priority_recommendations(shap_values, risk_level):\n",
    "    recommendations = []\n",
    "    key_factors = list(shap_values.items())\n",
    "    \n",
    "    # Urban density recommendations\n",
    "    if \"urban_land_usage\" in shap_values:\n",
    "        if shap_values[\"urban_land_usage\"] > 0:\n",
    "            recommendations.append({\n",
    "                \"category\": \"Urban Planning\",\n",
    "                \"impact\": \"High urban density is increasing biodiversity risk\",\n",
    "                \"solutions\": [\n",
    "                    \"Implement vertical gardens and green facades\",\n",
    "                    \"Create pocket parks in dense areas\",\n",
    "                    \"Establish green corridors connecting existing natural spaces\",\n",
    "                    \"Incorporate biodiversity-friendly infrastructure in new developments\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # Green cover recommendations\n",
    "    if \"ndvi\" in shap_values:\n",
    "        if shap_values[\"ndvi\"] < 0:\n",
    "            recommendations.append({\n",
    "                \"category\": \"Vegetation\",\n",
    "                \"impact\": \"Low vegetation cover is affecting biodiversity\",\n",
    "                \"solutions\": [\n",
    "                    \"Preserve existing native vegetation\",\n",
    "                    \"Create urban forests with native species\",\n",
    "                    \"Implement green roof policies\",\n",
    "                    \"Develop urban wildlife corridors\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    # Water management recommendations\n",
    "    if \"water_coverage\" in shap_values:\n",
    "        if shap_values[\"water_coverage\"] < 0:\n",
    "            recommendations.append({\n",
    "                \"category\": \"Water Management\",\n",
    "                \"impact\": \"Limited water features affecting biodiversity\",\n",
    "                \"solutions\": [\n",
    "                    \"Create or restore wetland areas\",\n",
    "                    \"Implement sustainable urban drainage systems\",\n",
    "                    \"Develop blue corridors alongside green spaces\",\n",
    "                    \"Protect and enhance existing water bodies\"\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "formatted_risk_score = float(risk_score) if hasattr(risk_score, 'item') else risk_score\n",
    "\n",
    "# Format SHAP values for display\n",
    "formatted_shap = json.dumps(shap_analysis['shap_values'], indent=2)\n",
    "\n",
    "# Format location data\n",
    "location_str = f\"Latitude: {cache['latitude']}, Longitude: {cache['longitude']}\"\n",
    "        \n",
    "system_prompt = f\"\"\"\n",
    "You are an expert assistant in biodiversity and urban planning specializing in SHAP-based impact assessments. Your task is to analyze impact predictions, reason about key contributing factors, and provide actionable recommendations for minimizing biodiversity loss in urban areas.\n",
    "\n",
    "Analysis Structure\n",
    "Reasoning for Risk Score:\n",
    "\n",
    "Explain the primary factors contributing to the risk score.\n",
    "Quantify the impact of the top three SHAP features and their significance.\n",
    "Highlight interaction effects among features influencing biodiversity.\n",
    "Targeted Recommendations:\n",
    "\n",
    "Urban Planning: Propose infrastructure changes like optimal green corridors, biodiversity-friendly designs, and urban density adjustments.\n",
    "Vegetation Management: Suggest preserving or planting native vegetation and improving habitat connectivity.\n",
    "Water Management: Recommend blue-green infrastructure to restore and maintain ecological balance.\n",
    "Implementation Guidelines:\n",
    "\n",
    "Prioritize changes based on SHAP feature impact.\n",
    "Address feasibility for the specific location, considering constraints and opportunities.\n",
    "Suggest phased implementation and monitoring metrics.\n",
    "Quality Requirements\n",
    "Ground all suggestions in SHAP-based numerical evidence.\n",
    "Ensure solutions align with the location’s risk level and context.\n",
    "Deliver a concise summary of reasoning and recommendations.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\"\n",
    "Biodiversity Risk Assessment\n",
    "\n",
    "Score: {formatted_risk_score:.2f} ({get_risk_level(risk_score)})\n",
    "Location: Latitude: {cache['latitude']}, Longitude: {cache['longitude']}\n",
    "Use Case: {cache['use_case_type']}\n",
    "Request:\n",
    "Please analyze the risk factors and provide a concise summary of:\n",
    "\n",
    "Reasoning:\n",
    "\n",
    "Why does the region have this risk score?\n",
    "What are the top three influential SHAP features, their impacts, and interactions?\n",
    "Recommendations:\n",
    "\n",
    "Urban planning solutions for minimizing biodiversity loss.\n",
    "Suggestions for green corridor placement and connectivity.\n",
    "Proposals for native vegetation preservation and biodiversity-friendly infrastructure.\n",
    "Implementation Plan:\n",
    "\n",
    "Prioritize actions based on SHAP impacts.\n",
    "Provide phased strategies and monitoring guidelines.\n",
    "Deliver your response in the following structure:\n",
    "\n",
    "Risk Analysis Summary\n",
    "SHAP Impact Insights\n",
    "Actionable Recommendations\n",
    "Implementation and Monitoring\n",
    "\"\"\"\n",
    "\n",
    "res = generate(system_prompt,user_prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e59cbb53-53d8-4618-b6b4-c46d12c68d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38175822])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_score_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6e798a18-eb52-44f8-a8a8-a63f77914c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shap_values': {'land_use_type': -0.0448152283278025,\n",
       "  'temperature': -0.018244000393543854,\n",
       "  'longitude': -0.013338526311123469,\n",
       "  'water_coverage': 0.0026838123618429287,\n",
       "  'latitude': -0.0024881404592925088,\n",
       "  'ndvi': -0.0021252582829121088,\n",
       "  'urban_land_usage': -0.0009733150222182587,\n",
       "  'humidity': 0.0008662264032009992,\n",
       "  'precipitation': -0.0004439142014124292,\n",
       "  'species_richness': -4.1867731471293744e-05},\n",
       " 'explanation': {'positive_impact': ['water_coverage', 'humidity'],\n",
       "  'negative_impact': ['land_use_type',\n",
       "   'temperature',\n",
       "   'longitude',\n",
       "   'latitude',\n",
       "   'ndvi',\n",
       "   'urban_land_usage',\n",
       "   'precipitation',\n",
       "   'species_richness'],\n",
       "  'most_influential_features': ['land_use_type', 'temperature', 'longitude']}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f62a90-15e8-48ba-a67e-440e1a2e5196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 02:46:51,144 - INFO - Processing region 1/1: 11.96, 75.92\n",
      "2025-01-19 02:47:24,004 - ERROR - Error in primary green cover fetch: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([{'latitude': 11.96, 'longitude': 75.92, 'temperature': 16.79, 'precipitation': 23.52, 'humidity': 85, 'species_richness': 206, 'ndvi': 0.187446, 'urban_land_usage': 0.5, 'water_coverage': 0.13021229656277938, 'land_use_type': 'Agricultural'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = [\n",
    "    {\"lat\": 10.7449, \"lon\": 92.5000},  # New York City, USA\n",
    "]\n",
    "# Define possible land use types\n",
    "land_use_types = [\n",
    "    \"green-based use\",\n",
    "]\n",
    "dataset = create_dataset(cache)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8f80c-e154-4e67-bb6f-199fc79568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inference_data(input_data, label_encoders, scaler, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Preprocess input data for inference.\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): New input data for prediction.\n",
    "        label_encoders (dict): Dictionary of fitted LabelEncoders for categorical features.\n",
    "        scaler (StandardScaler): Fitted StandardScaler for numerical features.\n",
    "        categorical_columns (list): List of categorical feature names.\n",
    "        numerical_columns (list): List of numerical feature names.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed input data.\n",
    "    \"\"\"\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        if col in data.columns and col in label_encoders:\n",
    "            data[col] = label_encoders[col].transform(data[col])\n",
    "    \n",
    "    # Scale numerical features\n",
    "    if set(numerical_columns).issubset(data.columns):\n",
    "        data[numerical_columns] = scaler.transform(data[numerical_columns])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def predict(input_data, models, weights, label_encoders, scaler, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Perform inference on input data.\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): New input data for prediction.\n",
    "        models (dict): Dictionary of trained models.\n",
    "        weights (dict): Dictionary of model weights for ensemble.\n",
    "        label_encoders (dict): Fitted label encoders for categorical features.\n",
    "        scaler (StandardScaler): Fitted scaler for numerical features.\n",
    "        categorical_columns (list): List of categorical feature names.\n",
    "        numerical_columns (list): List of numerical feature names.\n",
    "    \n",
    "    Returns:\n",
    "        float: Final ensemble prediction for `risk_score`.\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_inference_data(input_data, label_encoders, scaler, categorical_columns, numerical_columns)\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    predictions = {\n",
    "        name: model.predict(processed_data) for name, model in models.items()\n",
    "    }\n",
    "    \n",
    "    # Compute weighted ensemble prediction\n",
    "    ensemble_prediction = sum(predictions[model] * weight for model, weight in weights.items())\n",
    "    return ensemble_prediction\n",
    "\n",
    "# Example Inference\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input data\n",
    "    input_data = pd.DataFrame({\n",
    "        \"latitude\": [34.0522],\n",
    "        \"longitude\": [-118.2437],\n",
    "        \"temperature\": [25],\n",
    "        \"precipitation\": [50],\n",
    "        \"humidity\": [\"moderate\"],\n",
    "        \"ndvi\": [0.6],\n",
    "        \"species_richness\": [\"medium\"],\n",
    "        \"urban_land_usage\": [0.7],\n",
    "        \"water_coverage\": [0.3],\n",
    "        \"land_use_type\": [\"urban home-type use\"]\n",
    "    })\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_columns = ['humidity', 'species_richness', 'land_use_type']\n",
    "    numerical_columns = ['latitude', 'longitude', 'temperature', 'precipitation', \n",
    "                         'ndvi', 'urban_land_usage', 'water_coverage']\n",
    "    \n",
    "    # Perform inference\n",
    "    risk_score_prediction = predict(\n",
    "        input_data=input_data,\n",
    "        models=loaded_models,\n",
    "        weights=loaded_weights,\n",
    "        label_encoders=loaded_label_encoders,\n",
    "        scaler=loaded_scaler,\n",
    "        categorical_columns=categorical_columns,\n",
    "        numerical_columns=numerical_columns\n",
    "    )\n",
    "    \n",
    "    print(f\"Predicted Risk Score: {risk_score_prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
